{"class_name": "Tokenizer", "config": {"num_words": 2, "filters": "", "lower": true, "split": " ", "char_level": false, "oov_token": "<oov>", "document_count": 1, "word_counts": "{}", "word_docs": "{}", "index_docs": "{}", "index_word": "{\"1\": \"<oov>\"}", "word_index": "{\"<oov>\": 1}"}}